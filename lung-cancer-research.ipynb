{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2882784,"sourceType":"datasetVersion","datasetId":1748489,"isSourceIdPinned":false},{"sourceId":4813218,"sourceType":"datasetVersion","datasetId":2787116,"isSourceIdPinned":false}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install opencv-python scikit-learn pandas numpy matplotlib tensorflow-addons\n# !pip install tensorflow==2.12 keras==2.12\n# !pip install transformers datasets torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-06-26T17:36:06.474037Z","iopub.execute_input":"2025-06-26T17:36:06.474276Z","iopub.status.idle":"2025-06-26T17:36:06.477889Z","shell.execute_reply.started":"2025-06-26T17:36:06.474248Z","shell.execute_reply":"2025-06-26T17:36:06.477234Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport kagglehub\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D, Multiply, GlobalAveragePooling2D, Dense, Input, Softmax, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:36:08.815178Z","iopub.execute_input":"2025-06-26T17:36:08.815927Z","iopub.status.idle":"2025-06-26T17:36:36.375222Z","shell.execute_reply.started":"2025-06-26T17:36:08.815905Z","shell.execute_reply":"2025-06-26T17:36:36.374648Z"}},"outputs":[{"name":"stderr","text":"2025-06-26 17:36:13.016058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750959373.549138      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750959373.683010      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load_dataset\nfrom PIL import Image\nimport requests\nfrom sklearn.metrics import classification_report\nimport torch\nfrom torch.optim import AdamW \nfrom transformers import ViTFeatureExtractor\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:36:41.024175Z","iopub.execute_input":"2025-06-26T17:36:41.025067Z","iopub.status.idle":"2025-06-26T17:37:03.515070Z","shell.execute_reply.started":"2025-06-26T17:36:41.025041Z","shell.execute_reply":"2025-06-26T17:37:03.514243Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"adityamahimkar/iqothnccd-lung-cancer-dataset\")\nprint(\"Path to dataset files:\", path)\npath = kagglehub.dataset_download(\"justinkirby/the-cancer-imaging-archive-lidcidri\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:17.981805Z","iopub.execute_input":"2025-06-26T17:37:17.982440Z","iopub.status.idle":"2025-06-26T17:37:18.210316Z","shell.execute_reply.started":"2025-06-26T17:37:17.982415Z","shell.execute_reply":"2025-06-26T17:37:18.209666Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/iqothnccd-lung-cancer-dataset\nPath to dataset files: /kaggle/input/the-cancer-imaging-archive-lidcidri\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"data_dir = '/kaggle/input/iqothnccd-lung-cancer-dataset/The IQ-OTHNCCD lung cancer dataset/The IQ-OTHNCCD lung cancer dataset'\ntest_dir = '/kaggle/input/iqothnccd-lung-cancer-dataset/Test cases'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:20.402032Z","iopub.execute_input":"2025-06-26T17:37:20.402625Z","iopub.status.idle":"2025-06-26T17:37:20.406204Z","shell.execute_reply.started":"2025-06-26T17:37:20.402585Z","shell.execute_reply":"2025-06-26T17:37:20.405538Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"img_size = (224, 224)\nbatch_size = 8\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2\n)\n\nval_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    data_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:23.039029Z","iopub.execute_input":"2025-06-26T17:37:23.039653Z","iopub.status.idle":"2025-06-26T17:37:25.519314Z","shell.execute_reply.started":"2025-06-26T17:37:23.039628Z","shell.execute_reply":"2025-06-26T17:37:25.518470Z"}},"outputs":[{"name":"stdout","text":"Found 878 images belonging to 3 classes.\nFound 219 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(train_generator.class_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:28.629009Z","iopub.execute_input":"2025-06-26T17:37:28.629624Z","iopub.status.idle":"2025-06-26T17:37:28.633680Z","shell.execute_reply.started":"2025-06-26T17:37:28.629569Z","shell.execute_reply":"2025-06-26T17:37:28.632906Z"}},"outputs":[{"name":"stdout","text":"{'Bengin cases': 0, 'Malignant cases': 1, 'Normal cases': 2}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\ny_train = train_generator.classes  \n\n#Compute class weightss\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\n\n# Convert to dict for Keras\nclass_weights_dict = dict(enumerate(class_weights))\n\nprint(\"Class Weights:\", class_weights_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:30.623036Z","iopub.execute_input":"2025-06-26T17:37:30.623336Z","iopub.status.idle":"2025-06-26T17:37:30.632879Z","shell.execute_reply.started":"2025-06-26T17:37:30.623313Z","shell.execute_reply":"2025-06-26T17:37:30.632165Z"}},"outputs":[{"name":"stdout","text":"Class Weights: {0: 3.048611111111111, 1: 0.651818856718634, 2: 0.8788788788788788}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom transformers import ViTImageProcessor\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\nfull_dataset = datasets.ImageFolder(root=data_dir)\n\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\ntrain_dataset.dataset.transform = train_transform\nval_dataset.dataset.transform = val_transform\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:33.582734Z","iopub.execute_input":"2025-06-26T17:37:33.583060Z","iopub.status.idle":"2025-06-26T17:37:35.113749Z","shell.execute_reply.started":"2025-06-26T17:37:33.583037Z","shell.execute_reply":"2025-06-26T17:37:35.113121Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6ad3ee19f84041a66db871b47ed4d1"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"num_classes = len(full_dataset.classes)\n\nmodel = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224-in21k',\n    num_labels=num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:45.876322Z","iopub.execute_input":"2025-06-26T17:37:45.877086Z","iopub.status.idle":"2025-06-26T17:37:47.939270Z","shell.execute_reply.started":"2025-06-26T17:37:45.877062Z","shell.execute_reply":"2025-06-26T17:37:47.938673Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff103dd4b7254dd69d03882770bdaedf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c619fef761e438888faad0671f5fc38"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import copy\n\nbest_val_loss = float('inf')\nbest_model_state = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:50.398293Z","iopub.execute_input":"2025-06-26T17:37:50.398878Z","iopub.status.idle":"2025-06-26T17:37:50.402402Z","shell.execute_reply.started":"2025-06-26T17:37:50.398850Z","shell.execute_reply":"2025-06-26T17:37:50.401588Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport copy\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\ntrain_labels = [full_dataset.samples[i][1] for i in train_dataset.indices]\n\n\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nbest_val_loss = float('inf')\nbest_model_state = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:37:56.399471Z","iopub.execute_input":"2025-06-26T17:37:56.400164Z","iopub.status.idle":"2025-06-26T17:37:56.732321Z","shell.execute_reply.started":"2025-06-26T17:37:56.400143Z","shell.execute_reply":"2025-06-26T17:37:56.731716Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for epoch in range(15):\n    model.train()\n    total_loss = 0\n    \n    for batch in train_loader:\n        inputs, labels = batch\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        outputs = model(inputs).logits\n        loss = loss_fn(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        \n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs).logits\n            loss = loss_fn(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    avg_val_loss = val_loss / len(val_loader)\n    accuracy = correct / total\n\n    print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}\")\n    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n    # Save best model callback\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = copy.deepcopy(model.state_dict())\n        print(\"✅ Best model updated.\")\n\ntorch.save(best_model_state, \"best_vit_model.pth\")\nprint(\"Best model saved to 'best_vit_model.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:38:00.515402Z","iopub.execute_input":"2025-06-26T17:38:00.515999Z","iopub.status.idle":"2025-06-26T17:48:12.522041Z","shell.execute_reply.started":"2025-06-26T17:38:00.515977Z","shell.execute_reply":"2025-06-26T17:48:12.521327Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 20.0034\nValidation Loss: 0.5446, Accuracy: 0.9000\n✅ Best model updated.\nEpoch 2, Train Loss: 9.5773\nValidation Loss: 0.4553, Accuracy: 0.9364\n✅ Best model updated.\nEpoch 3, Train Loss: 5.6990\nValidation Loss: 0.2080, Accuracy: 0.9636\n✅ Best model updated.\nEpoch 4, Train Loss: 3.3641\nValidation Loss: 0.1794, Accuracy: 0.9545\n✅ Best model updated.\nEpoch 5, Train Loss: 2.5548\nValidation Loss: 0.2909, Accuracy: 0.8773\nEpoch 6, Train Loss: 1.7609\nValidation Loss: 0.0886, Accuracy: 0.9909\n✅ Best model updated.\nEpoch 7, Train Loss: 1.2374\nValidation Loss: 0.0847, Accuracy: 0.9773\n✅ Best model updated.\nEpoch 8, Train Loss: 0.9194\nValidation Loss: 0.0834, Accuracy: 0.9818\n✅ Best model updated.\nEpoch 9, Train Loss: 1.0352\nValidation Loss: 0.1589, Accuracy: 0.9727\nEpoch 10, Train Loss: 0.6506\nValidation Loss: 0.0764, Accuracy: 0.9818\n✅ Best model updated.\nEpoch 11, Train Loss: 0.5371\nValidation Loss: 0.0719, Accuracy: 0.9909\n✅ Best model updated.\nEpoch 12, Train Loss: 0.4772\nValidation Loss: 0.0728, Accuracy: 0.9909\nEpoch 13, Train Loss: 0.4325\nValidation Loss: 0.0736, Accuracy: 0.9909\nEpoch 14, Train Loss: 0.3958\nValidation Loss: 0.0738, Accuracy: 0.9909\nEpoch 15, Train Loss: 0.3641\nValidation Loss: 0.0742, Accuracy: 0.9909\nBest model saved to 'best_vit_model.pth'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        logits = model(inputs).logits\n        preds = torch.argmax(logits, dim=-1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\naccuracy = accuracy_score(all_labels, all_preds)\nprint(f\"Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:48:45.627382Z","iopub.execute_input":"2025-06-26T17:48:45.628015Z","iopub.status.idle":"2025-06-26T17:48:49.621955Z","shell.execute_reply.started":"2025-06-26T17:48:45.627988Z","shell.execute_reply":"2025-06-26T17:48:49.621081Z"}},"outputs":[{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n   Bengin cases       0.97      0.97      0.97        31\nMalignant cases       1.00      1.00      1.00       104\n   Normal cases       0.99      0.99      0.99        85\n\n       accuracy                           0.99       220\n      macro avg       0.99      0.99      0.99       220\n   weighted avg       0.99      0.99      0.99       220\n\nAccuracy: 0.9909\n","output_type":"stream"}],"execution_count":15}]}