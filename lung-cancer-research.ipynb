{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2882784,"sourceType":"datasetVersion","datasetId":1748489,"isSourceIdPinned":false},{"sourceId":4813218,"sourceType":"datasetVersion","datasetId":2787116,"isSourceIdPinned":false}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install opencv-python scikit-learn pandas numpy matplotlib tensorflow-addons\n# !pip install tensorflow==2.12 keras==2.12\n# !pip install transformers datasets torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-06-21T19:15:59.021407Z","iopub.execute_input":"2025-06-21T19:15:59.021979Z","iopub.status.idle":"2025-06-21T19:15:59.026072Z","shell.execute_reply.started":"2025-06-21T19:15:59.021927Z","shell.execute_reply":"2025-06-21T19:15:59.025175Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport kagglehub\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import DenseNet121  # Changed to DenseNet121 for speed\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D, Multiply, GlobalAveragePooling2D, Dense, Input, Softmax, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:15:59.027289Z","iopub.execute_input":"2025-06-21T19:15:59.027503Z","iopub.status.idle":"2025-06-21T19:16:14.277965Z","shell.execute_reply.started":"2025-06-21T19:15:59.027479Z","shell.execute_reply":"2025-06-21T19:16:14.277395Z"}},"outputs":[{"name":"stderr","text":"2025-06-21 19:16:00.688483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750533360.859645      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750533360.910120      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import ViTImageProcessor, ViTForImageClassification\nimport torch\nfrom datasets import load_dataset\nfrom PIL import Image\nimport requests\nfrom sklearn.metrics import classification_report\nimport torch\nfrom torch.optim import AdamW \nfrom transformers import ViTFeatureExtractor\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:16:14.278589Z","iopub.execute_input":"2025-06-21T19:16:14.279015Z","iopub.status.idle":"2025-06-21T19:16:27.958255Z","shell.execute_reply.started":"2025-06-21T19:16:14.278996Z","shell.execute_reply":"2025-06-21T19:16:27.957691Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"adityamahimkar/iqothnccd-lung-cancer-dataset\")\nprint(\"Path to dataset files:\", path)\npath = kagglehub.dataset_download(\"justinkirby/the-cancer-imaging-archive-lidcidri\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:16:27.959551Z","iopub.execute_input":"2025-06-21T19:16:27.960109Z","iopub.status.idle":"2025-06-21T19:16:28.255279Z","shell.execute_reply.started":"2025-06-21T19:16:27.960090Z","shell.execute_reply":"2025-06-21T19:16:28.254709Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/iqothnccd-lung-cancer-dataset\nPath to dataset files: /kaggle/input/the-cancer-imaging-archive-lidcidri\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"data_dir = '/kaggle/input/iqothnccd-lung-cancer-dataset/The IQ-OTHNCCD lung cancer dataset/The IQ-OTHNCCD lung cancer dataset'\ntest_dir = '/kaggle/input/iqothnccd-lung-cancer-dataset/Test cases'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:16:28.255968Z","iopub.execute_input":"2025-06-21T19:16:28.256206Z","iopub.status.idle":"2025-06-21T19:16:28.259381Z","shell.execute_reply.started":"2025-06-21T19:16:28.256178Z","shell.execute_reply":"2025-06-21T19:16:28.258818Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg_size = (224, 224)\nbatch_size = 32\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2\n)\n\nval_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    data_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:16:28.259990Z","iopub.execute_input":"2025-06-21T19:16:28.260197Z","iopub.status.idle":"2025-06-21T19:16:29.191903Z","shell.execute_reply.started":"2025-06-21T19:16:28.260176Z","shell.execute_reply":"2025-06-21T19:16:29.191350Z"}},"outputs":[{"name":"stdout","text":"Found 878 images belonging to 3 classes.\nFound 219 images belonging to 3 classes.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(train_generator.class_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:16:29.192646Z","iopub.execute_input":"2025-06-21T19:16:29.192918Z","iopub.status.idle":"2025-06-21T19:16:29.197069Z","shell.execute_reply.started":"2025-06-21T19:16:29.192899Z","shell.execute_reply":"2025-06-21T19:16:29.196347Z"}},"outputs":[{"name":"stdout","text":"{'Bengin cases': 0, 'Malignant cases': 1, 'Normal cases': 2}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\ny_train = train_generator.classes  # This is a NumPy array of class indices\n\n# Step 2: Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\n\n# Step 3: Convert to dict for Keras\nclass_weights_dict = dict(enumerate(class_weights))\n\nprint(\"Class Weights:\", class_weights_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:40:29.904740Z","iopub.execute_input":"2025-06-21T19:40:29.905464Z","iopub.status.idle":"2025-06-21T19:40:29.911843Z","shell.execute_reply.started":"2025-06-21T19:40:29.905440Z","shell.execute_reply":"2025-06-21T19:40:29.911123Z"}},"outputs":[{"name":"stdout","text":"Class Weights: {0: 3.048611111111111, 1: 0.651818856718634, 2: 0.8788788788788788}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from torch.utils.data import random_split\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to ViT's input size\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\nfull_dataset = datasets.ImageFolder(root=data_dir)\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\ntrain_dataset.dataset.transform = train_transform\nval_dataset.dataset.transform = val_transform\n\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:40:37.947576Z","iopub.execute_input":"2025-06-21T19:40:37.948306Z","iopub.status.idle":"2025-06-21T19:40:39.670497Z","shell.execute_reply.started":"2025-06-21T19:40:37.948281Z","shell.execute_reply":"2025-06-21T19:40:39.669946Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import ViTForImageClassification\n\nnum_classes = len(full_dataset.classes)\n\nmodel = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224-in21k',\n    num_labels=num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:42:51.960828Z","iopub.execute_input":"2025-06-21T19:42:51.961525Z","iopub.status.idle":"2025-06-21T19:42:52.460532Z","shell.execute_reply.started":"2025-06-21T19:42:51.961502Z","shell.execute_reply":"2025-06-21T19:42:52.459983Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import copy\n\nbest_val_loss = float('inf')\nbest_model_state = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:42:55.143122Z","iopub.execute_input":"2025-06-21T19:42:55.143829Z","iopub.status.idle":"2025-06-21T19:42:55.148056Z","shell.execute_reply.started":"2025-06-21T19:42:55.143804Z","shell.execute_reply":"2025-06-21T19:42:55.147475Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport copy\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Get training labels from Subset indices\ntrain_labels = [full_dataset.samples[i][1] for i in train_dataset.indices]\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n# Now use it in your loss function\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nbest_val_loss = float('inf')\nbest_model_state = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:44:57.975573Z","iopub.execute_input":"2025-06-21T19:44:57.975873Z","iopub.status.idle":"2025-06-21T19:44:58.129728Z","shell.execute_reply.started":"2025-06-21T19:44:57.975855Z","shell.execute_reply":"2025-06-21T19:44:58.129222Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"for epoch in range(15):\n    model.train()\n    total_loss = 0\n    \n    for batch in train_loader:\n        inputs, labels = batch\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        outputs = model(inputs).logits\n        loss = loss_fn(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        \n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs).logits\n            loss = loss_fn(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    avg_val_loss = val_loss / len(val_loader)\n    accuracy = correct / total\n\n    print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}\")\n    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n    # =========================\n    # Save best model callback\n    # =========================\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_state = copy.deepcopy(model.state_dict())\n        print(\"✅ Best model updated.\")\n\n# Save the best model\ntorch.save(best_model_state, \"best_vit_model.pth\")\nprint(\"🎉 Best model saved to 'best_vit_model.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:45:24.465350Z","iopub.execute_input":"2025-06-21T19:45:24.465894Z","iopub.status.idle":"2025-06-21T19:53:52.201460Z","shell.execute_reply.started":"2025-06-21T19:45:24.465872Z","shell.execute_reply":"2025-06-21T19:53:52.200622Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Train Loss: 15.5547\nValidation Loss: 0.6115, Accuracy: 0.8693\n✅ Best model updated.\nEpoch 2, Train Loss: 8.8775\nValidation Loss: 0.4279, Accuracy: 0.9205\n✅ Best model updated.\nEpoch 3, Train Loss: 4.6475\nValidation Loss: 0.3052, Accuracy: 0.9489\n✅ Best model updated.\nEpoch 4, Train Loss: 2.1651\nValidation Loss: 0.2410, Accuracy: 0.9773\n✅ Best model updated.\nEpoch 5, Train Loss: 1.9446\nValidation Loss: 0.1983, Accuracy: 0.9830\n✅ Best model updated.\nEpoch 6, Train Loss: 1.0676\nValidation Loss: 0.1946, Accuracy: 0.9830\n✅ Best model updated.\nEpoch 7, Train Loss: 0.7979\nValidation Loss: 0.1946, Accuracy: 0.9830\nEpoch 8, Train Loss: 0.6717\nValidation Loss: 0.2010, Accuracy: 0.9773\nEpoch 9, Train Loss: 0.5896\nValidation Loss: 0.2014, Accuracy: 0.9773\nEpoch 10, Train Loss: 0.5287\nValidation Loss: 0.2023, Accuracy: 0.9773\nEpoch 11, Train Loss: 0.4794\nValidation Loss: 0.2051, Accuracy: 0.9773\nEpoch 12, Train Loss: 0.4393\nValidation Loss: 0.2074, Accuracy: 0.9773\nEpoch 13, Train Loss: 0.4052\nValidation Loss: 0.2100, Accuracy: 0.9773\nEpoch 14, Train Loss: 0.3750\nValidation Loss: 0.2119, Accuracy: 0.9773\nEpoch 15, Train Loss: 0.3484\nValidation Loss: 0.2141, Accuracy: 0.9773\n🎉 Best model saved to 'best_vit_model.pth'\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        logits = model(inputs).logits\n        preds = torch.argmax(logits, dim=-1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:54:55.584007Z","iopub.execute_input":"2025-06-21T19:54:55.584262Z","iopub.status.idle":"2025-06-21T19:54:58.847693Z","shell.execute_reply.started":"2025-06-21T19:54:55.584246Z","shell.execute_reply":"2025-06-21T19:54:58.846897Z"}},"outputs":[{"name":"stdout","text":"                 precision    recall  f1-score   support\n\n   Bengin cases       1.00      0.83      0.90        23\nMalignant cases       1.00      1.00      1.00        76\n   Normal cases       0.95      1.00      0.97        77\n\n       accuracy                           0.98       176\n      macro avg       0.98      0.94      0.96       176\n   weighted avg       0.98      0.98      0.98       176\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"torch.save(model.state_dict(), \"vit_lung_cancer_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T19:23:45.997664Z","iopub.execute_input":"2025-06-21T19:23:45.998505Z","iopub.status.idle":"2025-06-21T19:23:46.411965Z","shell.execute_reply.started":"2025-06-21T19:23:45.998485Z","shell.execute_reply":"2025-06-21T19:23:46.411373Z"}},"outputs":[],"execution_count":13}]}